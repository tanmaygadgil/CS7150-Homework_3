{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d326d7d",
      "metadata": {
        "id": "3d326d7d"
      },
      "source": [
        "This can be run [run on Google Colab using this link](https://colab.research.google.com/github/CS7150/CS7150-Homework_3/blob/main/HW3.1-Classifiers.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dd40e9d",
      "metadata": {
        "id": "4dd40e9d"
      },
      "source": [
        "## MNIST Classifiers (Convolutional Neural Networks and Fully Connected Networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea20290e",
      "metadata": {
        "id": "ea20290e"
      },
      "source": [
        "<b>Optional</b>: Installing Wandb to see cool analysis of you code. You can go through the documentation here. We will do it for this assignment to get a taste of the GPU and CPU utilizations. If this is creating problems to your code, please comment out all the wandb lines from the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0f63cd31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f63cd31",
        "outputId": "7a537a16-a3b6-43c8-b48c-bd0c9bff6c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=5dbf27d8ade48d83279b36983841bc8174d85c66f8783687291e251c87ef8b40\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.37 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.12\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "# Uncomment the below line to install wandb (optinal)\n",
        "!pip install wandb\n",
        "# Uncomment the below line to install torchinfo (https://github.com/TylerYep/torchinfo) [Mandatory]\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8211a22f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8211a22f",
        "outputId": "da1ee53f-0126-410a-c6f4-b7a556b5758e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2023-10-12 21:56:05--  https://cs7150.baulab.info/2022-Fall/data/mnist-classify.pth\n",
            "Resolving cs7150.baulab.info (cs7150.baulab.info)... 35.232.255.106\n",
            "Connecting to cs7150.baulab.info (cs7150.baulab.info)|35.232.255.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1078198 (1.0M)\n",
            "Saving to: ‘mnist-classify.pth’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  4% 36.6M 0s\n",
            "    50K .......... .......... .......... .......... ..........  9% 43.6M 0s\n",
            "   100K .......... .......... .......... .......... .......... 14% 42.8M 0s\n",
            "   150K .......... .......... .......... .......... .......... 18% 51.8M 0s\n",
            "   200K .......... .......... .......... .......... .......... 23% 49.7M 0s\n",
            "   250K .......... .......... .......... .......... .......... 28% 54.2M 0s\n",
            "   300K .......... .......... .......... .......... .......... 33% 65.4M 0s\n",
            "   350K .......... .......... .......... .......... .......... 37% 7.39M 0s\n",
            "   400K .......... .......... .......... .......... .......... 42% 52.5M 0s\n",
            "   450K .......... .......... .......... .......... .......... 47% 70.7M 0s\n",
            "   500K .......... .......... .......... .......... .......... 52% 68.9M 0s\n",
            "   550K .......... .......... .......... .......... .......... 56% 94.8M 0s\n",
            "   600K .......... .......... .......... .......... .......... 61%  284M 0s\n",
            "   650K .......... .......... .......... .......... .......... 66%  350M 0s\n",
            "   700K .......... .......... .......... .......... .......... 71%  387M 0s\n",
            "   750K .......... .......... .......... .......... .......... 75% 14.5M 0s\n",
            "   800K .......... .......... .......... .......... .......... 80% 54.2M 0s\n",
            "   850K .......... .......... .......... .......... .......... 85% 72.2M 0s\n",
            "   900K .......... .......... .......... .......... .......... 90% 76.8M 0s\n",
            "   950K .......... .......... .......... .......... .......... 94%  361M 0s\n",
            "  1000K .......... .......... .......... .......... .......... 99%  268M 0s\n",
            "  1050K ..                                                    100% 5.45T=0.02s\n",
            "\n",
            "2023-10-12 21:56:05 (44.8 MB/s) - ‘mnist-classify.pth’ saved [1078198/1078198]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "wget -N https://cs7150.baulab.info/2022-Fall/data/mnist-classify.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "621c2c89",
      "metadata": {
        "scrolled": true,
        "id": "621c2c89"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,random_split,Subset\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "from typing import List\n",
        "from collections import OrderedDict\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e05fd6b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "e05fd6b2",
        "outputId": "d4506687-95b0-433f-984c-54915d1040cc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231012_215619-h92tqszu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tanmaygadgil96/hw3.1-ConvNets/runs/h92tqszu' target=\"_blank\">happy-breeze-2</a></strong> to <a href='https://wandb.ai/tanmaygadgil96/hw3.1-ConvNets' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/tanmaygadgil96/hw3.1-ConvNets' target=\"_blank\">https://wandb.ai/tanmaygadgil96/hw3.1-ConvNets</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/tanmaygadgil96/hw3.1-ConvNets/runs/h92tqszu' target=\"_blank\">https://wandb.ai/tanmaygadgil96/hw3.1-ConvNets/runs/h92tqszu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tanmaygadgil96/hw3.1-ConvNets/runs/h92tqszu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f1e1177e380>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Create an account at https://wandb.ai/site and paste the api key here (optional)\n",
        "import wandb\n",
        "wandb.init(project=\"hw3.1-ConvNets\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ab5423",
      "metadata": {
        "id": "53ab5423"
      },
      "source": [
        "### Some helper functions to view network parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5233e6cb",
      "metadata": {
        "id": "5233e6cb"
      },
      "outputs": [],
      "source": [
        "def view_network_parameters(model):\n",
        "    # Visualise the number of parameters\n",
        "    tensor_list = list(model.state_dict().items())\n",
        "    total_parameters = 0\n",
        "    print('Model Summary\\n')\n",
        "    for layer_tensor_name, tensor in tensor_list:\n",
        "        total_parameters += int(torch.numel(tensor))\n",
        "        print('{}: {} elements'.format(layer_tensor_name, torch.numel(tensor)))\n",
        "    print(f'\\nTotal Trainable Parameters: {total_parameters}!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "13a073a7",
      "metadata": {
        "id": "13a073a7"
      },
      "outputs": [],
      "source": [
        "def view_network_shapes(model, input_shape):\n",
        "    print(summary(conv_net, input_size=input_shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d140bdae",
      "metadata": {
        "id": "d140bdae"
      },
      "source": [
        "### Fully Connected Network for Image Classification\n",
        "Let's build a simple fully connected network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ccf368fb",
      "metadata": {
        "id": "ccf368fb"
      },
      "outputs": [],
      "source": [
        "def simple_fc_net():\n",
        "    model = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(1*28*28,8*28*28),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(8*28*28,16*14*14),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16*14*14,32*7*7),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32*7*7,288),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(288,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,10),\n",
        "        nn.LogSoftmax())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8306548c",
      "metadata": {
        "id": "8306548c"
      },
      "outputs": [],
      "source": [
        "fc_net = simple_fc_net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f968a3c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f968a3c1",
        "outputId": "808eac0d-87ae-4006-8456-4f5eef0a1b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Summary\n",
            "\n",
            "1.weight: 4917248 elements\n",
            "1.bias: 6272 elements\n",
            "3.weight: 19668992 elements\n",
            "3.bias: 3136 elements\n",
            "5.weight: 4917248 elements\n",
            "5.bias: 1568 elements\n",
            "7.weight: 451584 elements\n",
            "7.bias: 288 elements\n",
            "9.weight: 18432 elements\n",
            "9.bias: 64 elements\n",
            "11.weight: 640 elements\n",
            "11.bias: 10 elements\n",
            "\n",
            "Total Trainable Parameters: 29985482!\n"
          ]
        }
      ],
      "source": [
        "view_network_parameters(fc_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "83eb4838",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83eb4838",
        "outputId": "0e3be627-6bf0-49f2-9e73-5331bf79258a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Summary\n",
            "\n",
            "1.weight: 4917248 elements\n",
            "1.bias: 6272 elements\n",
            "3.weight: 19668992 elements\n",
            "3.bias: 3136 elements\n",
            "5.weight: 4917248 elements\n",
            "5.bias: 1568 elements\n",
            "7.weight: 451584 elements\n",
            "7.bias: 288 elements\n",
            "9.weight: 18432 elements\n",
            "9.bias: 64 elements\n",
            "11.weight: 640 elements\n",
            "11.bias: 10 elements\n",
            "\n",
            "Total Trainable Parameters: 29985482!\n"
          ]
        }
      ],
      "source": [
        "view_network_parameters(fc_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a19d63c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a19d63c2",
        "outputId": "2c93d265-d394-4761-a275-a6a44b952aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Sequential                               [1, 10]                   --\n",
              "├─Flatten: 1-1                           [1, 784]                  --\n",
              "├─Linear: 1-2                            [1, 6272]                 4,923,520\n",
              "├─ReLU: 1-3                              [1, 6272]                 --\n",
              "├─Linear: 1-4                            [1, 3136]                 19,672,128\n",
              "├─ReLU: 1-5                              [1, 3136]                 --\n",
              "├─Linear: 1-6                            [1, 1568]                 4,918,816\n",
              "├─ReLU: 1-7                              [1, 1568]                 --\n",
              "├─Linear: 1-8                            [1, 288]                  451,872\n",
              "├─ReLU: 1-9                              [1, 288]                  --\n",
              "├─Linear: 1-10                           [1, 64]                   18,496\n",
              "├─ReLU: 1-11                             [1, 64]                   --\n",
              "├─Linear: 1-12                           [1, 10]                   650\n",
              "├─LogSoftmax: 1-13                       [1, 10]                   --\n",
              "==========================================================================================\n",
              "Total params: 29,985,482\n",
              "Trainable params: 29,985,482\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 29.99\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.09\n",
              "Params size (MB): 119.94\n",
              "Estimated Total Size (MB): 120.04\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "summary(fc_net, input_size=(1, 1, 28,28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25cfc411",
      "metadata": {
        "id": "25cfc411"
      },
      "source": [
        "<b>Exercise</b>: Now try to add different layers and see how the network parameters vary. Does adding layers reduce the parameters? Does the number of hidden neurons in the layers affect the total trainable parameters?\n",
        "\n",
        "<i>Add a few sentences on your observations while using various architectures</i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2775c829",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2775c829",
        "outputId": "fb37f03d-9711-4476-a3b5-73e336d95540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Sequential                               [1, 10]                   --\n",
              "├─Flatten: 1-1                           [1, 784]                  --\n",
              "├─Linear: 1-2                            [1, 6272]                 4,923,520\n",
              "├─ReLU: 1-3                              [1, 6272]                 --\n",
              "├─Linear: 1-4                            [1, 3136]                 19,672,128\n",
              "├─ReLU: 1-5                              [1, 3136]                 --\n",
              "├─Linear: 1-6                            [1, 3136]                 9,837,632\n",
              "├─ReLU: 1-7                              [1, 3136]                 --\n",
              "├─Linear: 1-8                            [1, 1568]                 4,918,816\n",
              "├─ReLU: 1-9                              [1, 1568]                 --\n",
              "├─Linear: 1-10                           [1, 288]                  451,872\n",
              "├─ReLU: 1-11                             [1, 288]                  --\n",
              "├─Linear: 1-12                           [1, 64]                   18,496\n",
              "├─ReLU: 1-13                             [1, 64]                   --\n",
              "├─Linear: 1-14                           [1, 10]                   650\n",
              "├─LogSoftmax: 1-15                       [1, 10]                   --\n",
              "==========================================================================================\n",
              "Total params: 39,823,114\n",
              "Trainable params: 39,823,114\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 39.82\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.12\n",
              "Params size (MB): 159.29\n",
              "Estimated Total Size (MB): 159.41\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Please type your answer here ...\n",
        "def new_fc_net():\n",
        "    model = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(1*28*28,8*28*28),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(8*28*28,16*14*14),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16*14*14,16*14*14),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16*14*14,32*7*7),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32*7*7,288),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(288,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,10),\n",
        "        nn.LogSoftmax())\n",
        "    return model\n",
        "fc_net_new = new_fc_net()\n",
        "\n",
        "summary(fc_net_new, input_size=(1,1,28,28))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "245bc885",
      "metadata": {
        "id": "245bc885"
      },
      "source": [
        "\n",
        "### Convolutional Neural Network for Image Classification\n",
        "Let's build a simple CNN to classify our images.\n",
        "<b> Exercise 3.1.1:</b> In the function below please add the conv/Relu/Maxpool layers to match the shape of FC-Net. Suppose at the some layer the FC-Net has `28*28*16` dimension, we want your conv_net to have `16 X 28 X 28` shape at the same numbered layer. <br>\n",
        "<b>Extra-credit:</b> Try not to use MaxPool2d !"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_out_kernel(input_dim, in_kernel=3, strides=1, padding=0):\n",
        "\n",
        "  return (input_dim - in_kernel + 2* padding)/(strides) + 1"
      ],
      "metadata": {
        "id": "eM71ZfMk8Bsh"
      },
      "id": "eM71ZfMk8Bsh",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_out_kernel(28, 3,1,1)"
      ],
      "metadata": {
        "id": "QH70Pzjl8sS0",
        "outputId": "c4581c0b-db4b-49fa-8dba-4ef24a12dc48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QH70Pzjl8sS0",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_out_kernel(28,2,2)"
      ],
      "metadata": {
        "id": "X4sLW8iQ9WxE",
        "outputId": "16c650ad-8826-48a5-cfbf-ac6026d13acc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "X4sLW8iQ9WxE",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_out_kernel(14,3,1,8)"
      ],
      "metadata": {
        "id": "3V2Ai7mb9gRa",
        "outputId": "6220b31f-1cdc-4559-8871-0882b6aa549f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3V2Ai7mb9gRa",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "65674742",
      "metadata": {
        "id": "65674742"
      },
      "outputs": [],
      "source": [
        "def simple_conv_net():\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(1,16,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        # TO-DO: Add layers below\n",
        "        nn.Conv2d(16,16,kernel_size=3,padding=8,stride=1),\n",
        "        nn.ReLU(),\n",
        "        # TO-DO, what will your shape be after you flatten? Fill it in place of None\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(28*28*16,64),\n",
        "        # Do not change the code below\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,10),\n",
        "        nn.LogSoftmax())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6bb06a8d",
      "metadata": {
        "id": "6bb06a8d"
      },
      "outputs": [],
      "source": [
        "conv_net = simple_conv_net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c6d6e8f0",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6d6e8f0",
        "outputId": "f8cdc134-bbab-4f06-b908-8e2d7c2ddf49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Summary\n",
            "\n",
            "0.weight: 144 elements\n",
            "0.bias: 16 elements\n",
            "3.weight: 2304 elements\n",
            "3.bias: 16 elements\n",
            "6.weight: 802816 elements\n",
            "6.bias: 64 elements\n",
            "8.weight: 640 elements\n",
            "8.bias: 10 elements\n",
            "\n",
            "Total Trainable Parameters: 806010!\n"
          ]
        }
      ],
      "source": [
        "view_network_parameters(conv_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "49ed1786",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49ed1786",
        "outputId": "750ed867-90c0-476e-8da0-0f7851d42fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "Sequential                               [1, 10]                   --\n",
            "├─Conv2d: 1-1                            [1, 16, 28, 28]           160\n",
            "├─ReLU: 1-2                              [1, 16, 28, 28]           --\n",
            "├─MaxPool2d: 1-3                         [1, 16, 14, 14]           --\n",
            "├─Conv2d: 1-4                            [1, 16, 28, 28]           2,320\n",
            "├─ReLU: 1-5                              [1, 16, 28, 28]           --\n",
            "├─Flatten: 1-6                           [1, 12544]                --\n",
            "├─Linear: 1-7                            [1, 64]                   802,880\n",
            "├─ReLU: 1-8                              [1, 64]                   --\n",
            "├─Linear: 1-9                            [1, 10]                   650\n",
            "├─LogSoftmax: 1-10                       [1, 10]                   --\n",
            "==========================================================================================\n",
            "Total params: 806,010\n",
            "Trainable params: 806,010\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 2.75\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.20\n",
            "Params size (MB): 3.22\n",
            "Estimated Total Size (MB): 3.43\n",
            "==========================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        }
      ],
      "source": [
        "view_network_shapes(conv_net, input_shape=(1,1,28,28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f577403",
      "metadata": {
        "id": "0f577403"
      },
      "source": [
        "<b>Exercise 3.1.2</b>: Why is the final layer a log softmax? What is a softmax function? Can we use ReLU instead of softmax? If yes, what would you do different? If not, tell us why. If you think there is a different answer, feel free to use this space to chart it down"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e530f3e2",
      "metadata": {
        "id": "e530f3e2"
      },
      "source": [
        "A softmax is a function that converts logits to probabilioty distributions. These distributions are what we use to picka  class for a classifier and to  Please type your answer here ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41ea2c8c",
      "metadata": {
        "id": "41ea2c8c"
      },
      "source": [
        "<b>Exercise 3.1.3</b>: What is the ratio of number of parameters of Conv-net to number of parameters of FC-Net <br>\n",
        "$\\frac{p_{conv-net}}{p_{fc-net}}$ = Fill your answer <br>\n",
        "Do you see the difference ?!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c6329e",
      "metadata": {
        "id": "b4c6329e"
      },
      "source": [
        "<b>Exercise 3.1.4</b>: Now try to add different layers and see how the network parameters vary. Does adding layers reduce the parameters? Does the number of hidden neurons in the layers affect the total trainable parameters? Use the `build_custom_fc_net` function given below. You do not have to understand the working of it.\n",
        "\n",
        "<i>Add a few sentences on your observations while using various architectures</i>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89d76f7",
      "metadata": {
        "scrolled": true,
        "id": "b89d76f7"
      },
      "outputs": [],
      "source": [
        "def build_custom_fc_net(inp_dim: int, out_dim: int, hidden_fc_dim: List[int]):\n",
        "    '''\n",
        "    Inputs :\n",
        "\n",
        "    inp_dim: Shape of the input dimensions (in MNIST case 28*28)\n",
        "    out_dim: Desired classification classes (in MNIST case 10)\n",
        "    hidden_fc_dim: List of the intermediate dimension shapes (list of integers). Try different values and see the shapes'\n",
        "\n",
        "    Return: nn.Sequential (final custom model)\n",
        "    '''\n",
        "    assert type(hidden_fc_dim) == list, \"Please define hidden_fc_dim as list of integers\"\n",
        "    layers = []\n",
        "    layers.append((f'flatten', nn.Flatten()))\n",
        "    # If no hidden layer is required\n",
        "    if len(hidden_fc_dim) == 0:\n",
        "        layers.append((f'linear',nn.Linear(math.prod(inp_dim),out_dim)))\n",
        "        layers.append((f'activation',nn.LogSoftmax()))\n",
        "    else:\n",
        "        # Loop over hidden dimensions and add layers\n",
        "        for idx, dim in enumerate(hidden_fc_dim):\n",
        "            if idx == 0:\n",
        "                layers.append((f'linear_{idx+1}',nn.Linear(math.prod(inp_dim),dim)))\n",
        "                layers.append((f'activation_{idx+1}',nn.ReLU()))\n",
        "            else:\n",
        "                layers.append((f'linear_{idx+1}',nn.Linear(hidden_fc_dim[idx-1],dim)))\n",
        "                layers.append((f'activation_{idx+1}',nn.ReLU()))\n",
        "        layers.append((f'linear_{idx+2}',nn.Linear(dim,out_dim)))\n",
        "        layers.append((f'activation_{idx+2}',nn.LogSoftmax()))\n",
        "\n",
        "    model =  nn.Sequential(OrderedDict(layers))\n",
        "    return model\n",
        "\n",
        "# TO-DO build different networks (atleast 3) and see the parameters\n",
        "#(You don't have to understand the function above. It is a generic way to build a FC-Net)\n",
        "\n",
        "\n",
        "fc_net_custom1 = build_custom_fc_net(inp_dim=(1,28,28), out_dim=10, hidden_fc_dim=[128,64,32])\n",
        "view_network_parameters(fc_net_custom1)\n",
        "\n",
        "# fc_net_custom2 =\n",
        "# view_network_parameters(fc_net_custom2)\n",
        "\n",
        "# fc_net_custom3 =\n",
        "# view_network_parameters(fc_net_custom3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c48647",
      "metadata": {
        "id": "71c48647"
      },
      "source": [
        "## Let's train the models to see their performace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ae412d",
      "metadata": {
        "id": "94ae412d"
      },
      "outputs": [],
      "source": [
        "# downloading mnist into folder\n",
        "data_dir = 'data' # make sure that this folder is created in your working dir\n",
        "# transform the PIL images to tensor using torchvision.transform.toTensor method\n",
        "train_data = torchvision.datasets.MNIST(data_dir, train=True, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))\n",
        "test_data  = torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))\n",
        "print(f'Datatype of the dataset object: {type(train_data)}')\n",
        "# check the length of dataset\n",
        "n_train_samples = len(train_data)\n",
        "print(f'Number of samples in training data: {len(train_data)}')\n",
        "print(f'Number of samples in test data: {len(test_data)}')\n",
        "# Check the format of dataset\n",
        "#print(f'Foramt of the dataset: \\n {train_data}')\n",
        "\n",
        "val_split = .2\n",
        "batch_size=256\n",
        "\n",
        "train_data_, val_data = random_split(train_data, [int(n_train_samples*(1-val_split)), int(n_train_samples*val_split)])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data_, batch_size=batch_size,shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b94c083",
      "metadata": {
        "id": "0b94c083"
      },
      "source": [
        "### Displaying the loaded dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e15476",
      "metadata": {
        "id": "a6e15476"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2, 3, i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(train_data[i][0][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Class Label: {}\".format(train_data[i][1]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "524851bc",
      "metadata": {
        "id": "524851bc"
      },
      "source": [
        "## Function to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "718c5a83",
      "metadata": {
        "id": "718c5a83"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, device, loss_fn, optimizer, input_dim=(-1,1,28,28)):\n",
        "    model.train()\n",
        "    # Initiate a loss monitor\n",
        "    train_loss = []\n",
        "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning and not supervised classification)\n",
        "    for images, labels in train_loader: # the variable `labels` will be used for customised training\n",
        "        # reshape input\n",
        "        images = torch.reshape(images,input_dim)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # predict the class\n",
        "        predicted = model(images)\n",
        "        loss = loss_fn(predicted, labels)\n",
        "        # Backward pass (back propagation)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        wandb.log({\"Training Loss\": loss})\n",
        "        wandb.watch(model)\n",
        "        train_loss.append(loss.detach().cpu().numpy())\n",
        "    return np.mean(train_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "802d74a4",
      "metadata": {
        "id": "802d74a4"
      },
      "source": [
        "## Function to test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f24a06a",
      "metadata": {
        "id": "9f24a06a"
      },
      "outputs": [],
      "source": [
        "# Testing Function\n",
        "def test_model(model, test_loader, device, loss_fn, input_dim=(-1,1,28,28)):\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    model.eval()\n",
        "    with torch.no_grad(): # No need to track the gradients\n",
        "        # Define the lists to store the outputs for each batch\n",
        "        predicted = []\n",
        "        actual = []\n",
        "        for images, labels in test_loader:\n",
        "            # reshape input\n",
        "            images = torch.reshape(images,input_dim)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            ## predict the label\n",
        "            pred = model(images)\n",
        "            # Append the network output and the original image to the lists\n",
        "            predicted.append(pred.cpu())\n",
        "            actual.append(labels.cpu())\n",
        "        # Create a single tensor with all the values in the lists\n",
        "        predicted = torch.cat(predicted)\n",
        "        actual = torch.cat(actual)\n",
        "        # Evaluate global loss\n",
        "        val_loss = loss_fn(predicted, actual)\n",
        "    return val_loss.data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfdd1864",
      "metadata": {
        "id": "bfdd1864"
      },
      "source": [
        "Before we start training let's delete the huge FC-Net we built and build a reasonable FC-Net (You learnt why such larger networks are not reasonable in the previous notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51fca7cc",
      "metadata": {
        "id": "51fca7cc"
      },
      "outputs": [],
      "source": [
        "del fc_net, fc_net_custom1, fc_net_custom2, fc_net_custom3\n",
        "torch.cuda.empty_cache()\n",
        "# Building a reasonable fully connected network\n",
        "fc_net = build_custom_fc_net(inp_dim=(1,28,28), out_dim=10, hidden_fc_dim=[128,64,32])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bc501cf",
      "metadata": {
        "id": "6bc501cf"
      },
      "source": [
        "<b>Exercise 3.1.5:</b>\n",
        "Code the `weight_init_xavier` function by referring to https://pytorch.org/docs/stable/nn.init.html. Replace the weight initializations to your own function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60636ac1",
      "metadata": {
        "id": "60636ac1"
      },
      "outputs": [],
      "source": [
        "### Set the random seed for reproducible results\n",
        "torch.manual_seed(0)\n",
        "# Choosing a device based on the env and torch setup\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Selected device: {device}')\n",
        "\n",
        "def weight_init_zero(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.constant_(m.weight, 0.0)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "def weight_init_xavier(m):\n",
        "    '''\n",
        "    TO-DO: please add code below to add xavier uniform initialization and remove the 'pass'\n",
        "    '''\n",
        "    pass\n",
        "\n",
        "\n",
        "fc_net.to(device)\n",
        "conv_net.to(device)\n",
        "\n",
        "# Apply the weight initialization\n",
        "fc_net.apply(weight_init_zero)\n",
        "conv_net.apply(weight_init_zero)\n",
        "\n",
        "# Apply the xavier weight initialization\n",
        "#TO-DO: Add your function here\n",
        "fc_net.apply(weight_init_xavier)\n",
        "conv_net.apply(weight_init_xavier)\n",
        "\n",
        "\n",
        "# Take the parameters for optimiser\n",
        "params_to_optimize_fc = [\n",
        "    {'params': fc_net.parameters()}\n",
        "]\n",
        "\n",
        "params_to_optimize_conv = [\n",
        "    {'params': conv_net.parameters()}\n",
        "]\n",
        "### Define the loss function\n",
        "loss_fn = torch.nn.NLLLoss()\n",
        "### Define an optimizer (both for the encoder and the decoder!)\n",
        "lr= 0.001\n",
        "\n",
        "optim_fc = torch.optim.Adam(params_to_optimize_fc, lr=lr, weight_decay=1e-05)\n",
        "optim_conv = torch.optim.Adam(params_to_optimize_conv, lr=lr, weight_decay=1e-05)\n",
        "num_epochs = 30\n",
        "wandb.config = {\n",
        "  \"learning_rate\": lr,\n",
        "  \"epochs\": num_epochs,\n",
        "  \"batch_size\": batch_size\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53753ff1",
      "metadata": {
        "id": "53753ff1"
      },
      "source": [
        "# Training the Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9616b62",
      "metadata": {
        "scrolled": true,
        "id": "a9616b62"
      },
      "outputs": [],
      "source": [
        "print('Conv Net training started')\n",
        "history_conv = {'train_loss':[],'val_loss':[]}\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    ### Training\n",
        "\n",
        "    train_loss = train_model(\n",
        "        model=conv_net,\n",
        "        train_loader=train_loader,\n",
        "        device=device,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optim_conv,\n",
        "        input_dim=(-1,1,28,28))\n",
        "    ### Validation  (use the testing function)\n",
        "    val_loss = test_model(\n",
        "        model=conv_net,\n",
        "        test_loader=test_loader,\n",
        "        device=device,\n",
        "        loss_fn=loss_fn,\n",
        "        input_dim=(-1,1,28,28))\n",
        "    # Print Losses\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} : train loss {train_loss:.3f} \\t val loss {val_loss:.3f}')\n",
        "    history_conv['train_loss'].append(train_loss)\n",
        "    history_conv['val_loss'].append(val_loss)\n",
        "\n",
        "\n",
        "print(f'Conv Net training done in {(datetime.datetime.now()-start_time).total_seconds():.3f} seconds!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1465fbd",
      "metadata": {
        "id": "f1465fbd"
      },
      "source": [
        "### Visualizing Training Progress of Conv Net (Also check out your wandb.ai homepage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1622763",
      "metadata": {
        "id": "c1622763"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "plt.plot(history_conv['train_loss'], color='blue')\n",
        "plt.plot(history_conv['val_loss'], color='red')\n",
        "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Negative Log Likelihood Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f163e64a",
      "metadata": {
        "id": "f163e64a"
      },
      "source": [
        "### Visualizing Predictions of Conv Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89ac3520",
      "metadata": {
        "scrolled": false,
        "id": "89ac3520"
      },
      "outputs": [],
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "with torch.no_grad():\n",
        "    example_data = example_data.to(device)\n",
        "    output = conv_net(example_data)\n",
        "example_data = example_data.cpu().detach().numpy()\n",
        "fig = plt.figure(figsize=(5,5))\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(example_data[i][0], cmap='gray',interpolation='none')\n",
        "    plt.title(\"Prediction: {}\".format(\n",
        "    output.data.max(1, keepdim=True)[1][i].item()))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eacd45fe",
      "metadata": {
        "id": "eacd45fe"
      },
      "source": [
        "# Training the Fully-Connected Neural Networks\n",
        "\n",
        "<b>Exercise 3.1.6:</b> Train the fully connected neural network and analyse it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee8d12c",
      "metadata": {
        "id": "4ee8d12c"
      },
      "outputs": [],
      "source": [
        "#TO-DO:Train the fc_net here\n",
        "print('FC Net training started')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c5cd18",
      "metadata": {
        "id": "b4c5cd18"
      },
      "source": [
        "## Visualizing Training Progress of FC Net (Check out your wandb.ai project webpage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "926d5e38",
      "metadata": {
        "id": "926d5e38"
      },
      "outputs": [],
      "source": [
        "# TODO - Visualize the training progress of fc_net\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "107c77be",
      "metadata": {
        "id": "107c77be"
      },
      "source": [
        "## Visualizing Predictions of FC Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f399e8",
      "metadata": {
        "id": "a6f399e8"
      },
      "outputs": [],
      "source": [
        "# TODO - Visualise the predictions of fc_net\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2981154",
      "metadata": {
        "id": "b2981154"
      },
      "source": [
        "<b>Exercise 3.1.7</b>: What are the training times for each of the model? Did both the models take similar times? If yes, why? Shouldn't CNN train faster given it's number of weights to train?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3528f2d9",
      "metadata": {
        "id": "3528f2d9"
      },
      "outputs": [],
      "source": [
        "#Please type your answer here ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b33605f1",
      "metadata": {
        "id": "b33605f1"
      },
      "source": [
        "## Let's see how the models perform under translation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a4d24b6",
      "metadata": {
        "id": "7a4d24b6"
      },
      "source": [
        "In principle, one of the advantages of convolutions is that they are equivariant under translation which means that a function composed out of convolutions should invariant under translation.\n",
        "\n",
        "<b>Exercise 3.1.8</b>: In practice, however, we might not see perfect invariance under translation.  What aspect of our network leads to imperfect invariance?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "889cbdfc",
      "metadata": {
        "id": "889cbdfc"
      },
      "source": [
        "Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8a0d35b",
      "metadata": {
        "id": "b8a0d35b"
      },
      "source": [
        "We will next measure the sensitivity  of the convolutional network to translation in practice, and we will compare it to the fully-connected version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e8583d7",
      "metadata": {
        "id": "9e8583d7"
      },
      "outputs": [],
      "source": [
        "## function to check accuracies for unit translation\n",
        "def shiftVsAccuracy(model, test_loader, device, loss_fn, shifts = 12, input_dim=(-1,1,28,28)):\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    accuracies = []\n",
        "    shifted = []\n",
        "    for i in range(-shifts,shifts):\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad(): # No need to track the gradients\n",
        "            # Define the lists to store the outputs for each batch\n",
        "            predicted = []\n",
        "            actual = []\n",
        "            for images, labels in test_loader:\n",
        "                # reshape input\n",
        "                images = torch.roll(images,shifts=i, dims=2)\n",
        "                if i == 0:\n",
        "                    pass\n",
        "                elif i > 0:\n",
        "                    images[:,:,:i,:] = 0\n",
        "                else:\n",
        "                    images[:,:,i:,:] = 0\n",
        "                images = torch.reshape(images,input_dim)\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                ## predict the label\n",
        "                pred = model(images)\n",
        "                # Append the network output and the original image to the lists\n",
        "                _ , pred = torch.max(pred.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (pred == labels).sum().item()\n",
        "                predicted.append(pred.cpu())\n",
        "                actual.append(labels.cpu())\n",
        "            shifted.append(images[0][0].cpu())\n",
        "            acc = 100 * correct // total\n",
        "            accuracies.append(acc)\n",
        "    return accuracies,shifted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89576a2d",
      "metadata": {
        "id": "89576a2d"
      },
      "outputs": [],
      "source": [
        "accuracies,shifted = shiftVsAccuracy(\n",
        "        model=conv_net,\n",
        "        test_loader=test_loader,\n",
        "        device=device,\n",
        "        shifts=12,\n",
        "        loss_fn=loss_fn,\n",
        "        input_dim=(-1,1,28,28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e2b0c9",
      "metadata": {
        "id": "99e2b0c9"
      },
      "outputs": [],
      "source": [
        "shifts = np.arange(-12,12)\n",
        "plt.plot(shifts,accuracies)\n",
        "plt.title('Accuracy Vs Translation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2320b45",
      "metadata": {
        "id": "b2320b45"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(20,20))\n",
        "plt_num = 0\n",
        "for i in range(-12,12):\n",
        "    plt.subplot(5,6,plt_num+1)\n",
        "    plt.imshow(shifted[plt_num], cmap='gray',interpolation='none')\n",
        "    plt.title(f\"Shifted: {i} Accuracy: {accuracies[plt_num]}\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt_num+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b217e71",
      "metadata": {
        "id": "6b217e71"
      },
      "source": [
        "<b>Exercise 3.1.8:</b>\n",
        "Do the same for FC-Net and plot the accuracies. Is the rate of accuracy degradation same as Conv-Net? Can you justify why this happened? <br>\n",
        "Clue: You might want to look at the way convolution layers process information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b4cf9a",
      "metadata": {
        "id": "80b4cf9a"
      },
      "outputs": [],
      "source": [
        "# To-DO Write your code below"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}